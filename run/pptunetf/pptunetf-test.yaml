# DCASE2021 preprocessing tuning transformer system(test)
dev_directory:    ../dev_data
eval_directory:   ../eval_data
model_directory:  ../model/{model}/{config}/{feature}/{model_for}/{net}/{fit}/{eval}
result_directory: ../result/{model}/{config}/{feature}/{model_for}/{net}/{fit}/{eval}/{decision}
summary_ods:      ../result/{model}/{config}/_summary_{model}_{config}.ods
summary_csv:      ../result/{model}/{config}/_summary_{model}_{config}.csv
summary_xlsx:      ../result/{model}/{config}/_summary_{model}_{config}.xlsx
result_file:      _result_{model}_{config}_{feature}_{model_for}_{net}_{fit}_{eval}_{decision}.csv

transfer_learning:
  # ?]??w?K??????
  model_directory:  ../model/{model}/{config}/{feature}/{model_for}/{net}/{fit}/{eval}/{tl_fit}/{tl_eval}
  result_directory: ../result/{model}/{config}/{feature}/{model_for}/{net}/{fit}/{eval}/{tl_fit}/{tl_eval}/{decision}
  result_file:      _result_{model}_{config}_{feature}_{model_for}_{net}_{fit}_{eval}_{tl_fit}_{tl_eval}_{decision}.csv

  # ?]??w?K???????(target='target')
  tl_fit:
    dropout: 0.1
    lr: 1.0e-04          # param. tuning learning rate('--tl_lr' ??w??)
    val_split: 0.0       # ????f?[?^?????0.0?????(???)
    batch_size: 8        # ('--tl_batch_size ??w??')
    #                    # ?f?[?^?g??
    aug_count: 0         # ??????????{??(????{???????t?@?C???????????)(0????????????)(eg. 10)
                         # ????????A??????`??g??????s??
    aug_gadd: 0.0        # ???0?`aug_gadd????K???????d???(eg. 3.0e-4)
    aug_wcut: 0          # ?O??v???????g?`???????_??????(????t???[????u???????)(eg. 1024)

  tl_eval:
    epochs: 3 # 32       # domain adaptation training epochs ('--tl_epochs' ??w??)
    eval_epoch: best     # ?]????g?????f?? { best | <num> | last } ('--tl_eval_epoch' ??w??)

net:
  model: pptunetf        # DCASE2021  preprocessing tuning transformer system
  n_enc_l: 2             # ?K?w?? num encoder layers
  nhead: 4               # number of heads in attention
  d_model : 16 # 128     # transformer data dimension
  d_ff: 64 # 512         # transformer feed forward dimension
  attn_type: linear      # eg. linear, causal-linear, ...
  inp_layer: non-linear  # ????w(linear, none, non-linear)
  out_layer: linear      # ?o??w
  pos_enc: none          # position encoding(none, embed, sin_cos)

model_for:               # domain ?????
  f_machine: 1           # machine_type ???
  f_section: 1           # section_index ???
  f_target: pp-raw-tf3   # pp-raw-tf6   # domain ???

decision:
  max_fpr: 0.1
  decision_thr: 0.9

feature:
  n_mels: 16 #128
  n_frames: 5 # 16           # used as the sequence length for transformer layers
  n_hop_frames: 8 # 1
  n_fft: 1024
  hop_length: 4096 #512
  power: 2.0

fit:
  dropout: 0.1
  lr: 1.0e-05              # learning rate
  batch_size: 16 
  shuffle: 1
  val_split: 0.1
  data_size: 20 # -1          # ?w?K?f?[?^??????, '-1' to use all data
  # ?f?[?^?g??(???w?K)
  aug_count: 0           # ??????????{??(????{???????t?@?C???????????)(0????????????)(eg. 10)
                         # ????????A??????`??g??????s??
  aug_gadd: 0.0          # ???0?`aug_gadd????K???????d???(eg. 3.0e-4)
  aug_wcut: 0            # ?O??v???????g?`???????_??????(????t???[????u???????)(eg. 1024)

eval:
  epochs: 2 # 10             # number of epochs
  eval_epoch: best       # ?]????g?????f?? { best | <num> | last } 

misc: # these items do not affect evaluation result
  batch_size: 8           # batch size while evaluation
  shuffle: False          # shuffle while evaluation
  save_freq: 10           # ????p?x
  overwrite: 0            # for debug
  basedata_memory: extend # basedata???????????f?[?^(compact[?x??] or extend[????])
  restart_train: 1        # ?r???????J
  config: test

limit:
  usable_machine_types: ['ToyCar','ToyTrain','fan', 'gearbox','pump','slider','valve']
